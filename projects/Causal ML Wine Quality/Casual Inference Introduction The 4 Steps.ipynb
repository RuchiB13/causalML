{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wgu-g34WmWh"
   },
   "source": [
    "# Causal Inference Introduction\n",
    " \n",
    "**Ruchi Bhavsar, Darshil Patel, and Andrew Lemke**\n",
    " \n",
    "**Causal Machine Learning Summer 2022**\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import dowhy\n",
    "from dowhy import CausalModel\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wgu-g34WmWh"
   },
   "source": [
    " \n",
    "# What is causal inference and what does it do?\n",
    " \n",
    "Describing the goal of causal inference in relation to traditional (predictive) modeling highlights the key difference. _In most data science modeling, prediction is the goal_--given a sample X, predict some output y. Usually the predictor is some machine learning (ML) model. The data scientist has many samples of X and their corresponding y's, and whether the model be some regression or classification, the data scientist uses these samples and some ML learning scheme to build a model that gives an output for some X where this output matches the true output for many observed samples. The predictor could even be entirely black boxed and still satisfy many needs.\n",
    " \n",
    "Causal models seek to explain _how the data was generated_--we are making a generative model not a predictive model. With causal inference, we are modeling the process that creates the data, explaining how a certain observation (X, y) came to be. Causal reasoning is more natural for humans; we make our predictions based on known causes and effects. Causal models have the ability to generalize better and can give insight to why a prediction is made.\n",
    " \n",
    " \n",
    " \n",
    "## DAGs and causal inference\n",
    " \n",
    "Directed acyclic graphs (DAGs) can be used as a representation of this generating process. Nodes hold the variables in the system, and edges define the causal relationships.\n",
    " \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1svpZNKJoj7ERdxK3gIfkPS8Qy2n4vu_b\"  width=\"400\">\n",
    " \n",
    "Here is an example of a DAG that might be constructed for a company looking to model their advertising on a popular video streaming site.\n",
    " \n",
    "The DAG can be used to generate samples. Along with the actual output of the model, the DAG also gives us the values of all the nodes that cause the output. The DAG itself gives us insights to why things happen. When we model causality with a DAG, we naturally get conditional independence between variables in the system. In the example above, we can extract information like P(sale | Ad Type) or p(Sale|Times seen). More importantly, we can simulate interventions, allowing us to see from the data how showing ads to people multiple times influences our (statistical) expectation of sales. A predictive model does not have this capability.\n",
    " \n",
    "Where do you get these probabilities? In most cases, they are learned from data. This process can occur from some dataset that has attributes for all the nodes. We are not limited to just this method however. If we know from industry that the probability that a website visitor has a 0.019 chance of digesting the entirety of an arbitrary advertisement, then we can use this in our DAG. We can also incorporate known knowledge in other ways. If we set up our advertisements in the above example such that each advertisement has a different promotional level, then we can set those nodes accordingly.\n",
    " \n",
    " \n",
    "## Correlation and Causality\n",
    " \n",
    "You might be wondering, \"If we just learned these causal relationships from observational data, wouldn't we have used correlation to derive causation, going against the established fact that \"correlation does not imply causation\"\"? In a way we are, but it is not problematic. \n",
    " \n",
    "The first reason is that \"correlation does not imply causation\" isn't the whole story. Richenbach's common cause principle holds that if we observe that two things that are correlated, one either causes the other or they share some common cause. When we look at observational data, domain knowledge (or just reason) allows us to narrow down this further. For example, if we see a correlation between bank customers who spend high amounts of time on hold and bank customers who then leave the bank, we can easily reason that customers leaving the bank does not cause their high weight times in the past.\n",
    " \n",
    "The common cause bit is a little more tricky. Here, we must rely on domain knowledge to answer the question, \"does this study adequately account for possible common causes?\" If we think it does, then we can proceed to draw causal conclusions from our model.\n",
    " \n",
    " \n",
    "The second reason is that this is the best alternative. If the company advertising on videos hired pure statisticians, they would receive tables of correlations with significance levels and carefully crafted language that avoids calling things \"causes.\" For example, \"we see a high degree of association between promotional discount level and clicking on advertisement.\" The company would then use the statisticians' report to make business decisions as if they were causes, and increase their promotional level to draw more customers. What causal modeling offers is a wrong model, but a useful one. The 5 non-outcome nodes in the above example are far from what is needed to capture all the decision making processes of each person exposed to an ad. The point of modeling is not to  account for every factor to make it correct. Modeling is about usefulness, and in situations where one cannot perform a controlled A/B experiment (the gold standard of causality), a causal model is often the best choice.\n",
    " \n",
    " \n",
    "## Clearing up some vocabulary\n",
    " \n",
    "A **generative model** enumerates the joint probability distribution of variables in the data generating process. It is named \"generative\" because once we have the joint probability, we can generate new samples.\n",
    " \n",
    "A **discriminative model** predicts some output variable(s) from the data. This is also called a predictive model. Examples would include a neural net.\n",
    " \n",
    "A **probabilistic model** is a model that includes probabilities in its output. This could be giving a probability distribution for output rather than a single point or class. Logistic regression is a probabilistic model while SVM is not as it just outputs the class.\n",
    " \n",
    "A **Bayesian model** is a probabilistic model that handles uncertainty in not just outputs but internal elements as well, including the weights themselves.\n",
    " \n",
    " \n",
    "# The four steps of causal inference\n",
    " \n",
    "The four steps of causal inference are:\n",
    "1. Model\n",
    "2. Identify\n",
    "3. Estimate\n",
    "4. Refute\n",
    " \n",
    "We will first go over the four steps, then show some code examples in `DoWhy` following these steps.\n",
    " \n",
    " \n",
    "## 1. The Model\n",
    " \n",
    "The choice of model is chief in the causal inference process. The model must account for the necessary variables to represent the true causal relationship, but also not become bloated by unnecessary variables that do not impact our causal query. One simple rule is: for any set of 2 or more variables in our graph, we cannot have a common cause of those variables not be included in the graph.\n",
    " \n",
    " \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Fdz_O5BusckBzryvLQ2Nn6TR-cykgO4A\"  width=\"200\">\n",
    " \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1TZu6xcFLDh9uv6dx3RyX11LS-a6-HtAq\"  width=\"200\">\n",
    " \n",
    " \n",
    " \n",
    "In the first example our model includes the nodes in blue, but not node Z. We need to account for node Z, because without it, we do not acknowledge the dependency between X1 and X2 as they share a common cause. In other words, our model (in blue) implies independence between X1 and X2, but in truth (blue and gray nodes), there is no independence as X1 and X2 are d-connected.\n",
    " \n",
    "In the second example, we are safe to not include Z. Z does not d-connect X1 and X2.\n",
    " \n",
    " \n",
    "### Types of nodes\n",
    " \n",
    "Nodes can be classified based on their position in the graph. These classifications will be useful in the next step, identify. In the below example graph, we are trying to determine the causal effect of X on Y. We will explain each type of node in relation to this graph (ie X will be the treatment, Y will be the outcome). We wish to get $P(Y=y | do(X=x))$\n",
    " \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1guMGqxl8jvs9BrIBso67q_hLWGuG50xf\"  width=\"600\">\n",
    " \n",
    "* **Common Causes** - *Z1 above* These nodes have causal influence on both the treatment and the outcome. These nodes are also called confounders, as if we observe some correlation between X and Y, we cannot know if this is due to the effect of X on Y or due to association through Z1. Nodes like Z1 act as a \"backdoor\" of causal influence.\n",
    " \n",
    "* **Mediators** - *M above* While Z1 might be the backdoor, M is the frontdoor. Even if \n",
    " \n",
    "* **Instrument Variables (IVs)** - *Z2, Z3 above* These variables are causes for the treatment but not for the outcome on any path of causality not through X. In other words, their only causal path to Y is through X.  \n",
    " \n",
    " \n",
    " \n",
    "## **2. Identify**\n",
    " \n",
    " \n",
    "### The backdoor adjustment\n",
    "The backdoor method works if we know all the common causes of X and Y. If we know Z1 or all the nodes like it, then we can use the backdoor estimate.\n",
    " \n",
    " \n",
    "### The frontdoor adjustment\n",
    " \n",
    "In the same example, M acts as the mediator for causality through X. If we have a node like this, then we can use the frontdoor method. The reason why this works is because M captures the causal effect going from X to Y *not going through the backdoor*. Having an observed node like M allows us to calculate this effect.\n",
    " \n",
    " \n",
    "### Instrumental Variables \n",
    " \n",
    "Instrumental variable estimation is possible when IVs exist for X. Here is a brief explanation of why we can calculate the effect with mediators. If the backdoor (Z1) and the instruments (Z2, Z3) are independent of each other, then an estimator for Y based on the instruments would be equal to an estimator for Y based on the instruments and the back door. How do we know they are independent? They are not connected in the DAG. \n",
    " \n",
    "If we regress on the instruments, what we get is the effect on the instruments and X on Y. If we create another estimate of the effect of the IVs on X, which is not difficult as we are just looking at the simple causal relationship of Z2 -> X and Z3 -> X, then we can divide the estimated effect of instruments and X on y by the effect of the instruments, then we can get just the effect of X on Y. \n",
    " \n",
    "The largest benefit of this method is that we do not even need to know the backdoor values. If we know there is a common cause, but we do not have data for it, we can still use the IVs to get the effect of X on Y.\n",
    " \n",
    "### **Example**\n",
    "\n",
    "The following DAG has all three ways of identification. An example DoWhy Model is created from the DAG. DoWhy automatically solves identification, and its output confirms that each way is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "M_U_LRGCOx8a",
    "outputId": "cdd482b6-db5a-49c0-d663-738cd39ed100"
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\r\n",
       " -->\r\n",
       "<!-- Title: DAG Pages: 1 -->\r\n",
       "<svg width=\"278pt\" height=\"332pt\"\r\n",
       " viewBox=\"0.00 0.00 278.00 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\r\n",
       "<title>DAG</title>\r\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 274,-328 274,4 -4,4\"/>\r\n",
       "<!-- Y -->\r\n",
       "<g id=\"node1\" class=\"node\">\r\n",
       "<title>Y</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"121\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Y</text>\r\n",
       "</g>\r\n",
       "<!-- X -->\r\n",
       "<g id=\"node2\" class=\"node\">\r\n",
       "<title>X</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"121\" cy=\"-234\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-230.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\r\n",
       "</g>\r\n",
       "<!-- M1 -->\r\n",
       "<g id=\"node3\" class=\"node\">\r\n",
       "<title>M1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"121\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-158.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">M1</text>\r\n",
       "</g>\r\n",
       "<!-- X&#45;&gt;M1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\">\r\n",
       "<title>X&#45;&gt;M1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M121,-215.7C121,-207.98 121,-198.71 121,-190.11\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.5,-190.1 121,-180.1 117.5,-190.1 124.5,-190.1\"/>\r\n",
       "</g>\r\n",
       "<!-- M2 -->\r\n",
       "<g id=\"node4\" class=\"node\">\r\n",
       "<title>M2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"121\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"121\" y=\"-86.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">M2</text>\r\n",
       "</g>\r\n",
       "<!-- M1&#45;&gt;M2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\">\r\n",
       "<title>M1&#45;&gt;M2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M121,-143.7C121,-135.98 121,-126.71 121,-118.11\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.5,-118.1 121,-108.1 117.5,-118.1 124.5,-118.1\"/>\r\n",
       "</g>\r\n",
       "<!-- M2&#45;&gt;Y -->\r\n",
       "<g id=\"edge3\" class=\"edge\">\r\n",
       "<title>M2&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M121,-71.7C121,-63.98 121,-54.71 121,-46.11\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.5,-46.1 121,-36.1 117.5,-46.1 124.5,-46.1\"/>\r\n",
       "</g>\r\n",
       "<!-- C1 -->\r\n",
       "<g id=\"node5\" class=\"node\">\r\n",
       "<title>C1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">C1</text>\r\n",
       "</g>\r\n",
       "<!-- C1&#45;&gt;Y -->\r\n",
       "<g id=\"edge6\" class=\"edge\">\r\n",
       "<title>C1&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M29.15,-287.95C34.55,-248.51 50.81,-148.5 85,-72 89.56,-61.79 96.09,-51.43 102.34,-42.61\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"105.3,-44.49 108.41,-34.37 99.66,-40.34 105.3,-44.49\"/>\r\n",
       "</g>\r\n",
       "<!-- C1&#45;&gt;X -->\r\n",
       "<g id=\"edge4\" class=\"edge\">\r\n",
       "<title>C1&#45;&gt;X</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.68,-291.83C58.95,-281.21 79.22,-266.11 95.29,-254.15\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"97.79,-256.65 103.72,-247.87 93.61,-251.04 97.79,-256.65\"/>\r\n",
       "</g>\r\n",
       "<!-- C2 -->\r\n",
       "<g id=\"node6\" class=\"node\">\r\n",
       "<title>C2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"243\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">C2</text>\r\n",
       "</g>\r\n",
       "<!-- C2&#45;&gt;Y -->\r\n",
       "<g id=\"edge7\" class=\"edge\">\r\n",
       "<title>C2&#45;&gt;Y</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M238.29,-288.03C227.05,-248.75 196.65,-149.07 157,-72 151.89,-62.06 145.24,-51.76 139.05,-42.91\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"141.77,-40.69 133.09,-34.61 136.08,-44.77 141.77,-40.69\"/>\r\n",
       "</g>\r\n",
       "<!-- C2&#45;&gt;X -->\r\n",
       "<g id=\"edge5\" class=\"edge\">\r\n",
       "<title>C2&#45;&gt;X</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.13,-293.6C203.19,-282.16 172.27,-264.42 149.68,-251.46\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"151.28,-248.34 140.86,-246.4 147.8,-254.41 151.28,-248.34\"/>\r\n",
       "</g>\r\n",
       "<!-- I1 -->\r\n",
       "<g id=\"node7\" class=\"node\">\r\n",
       "<title>I1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">I1</text>\r\n",
       "</g>\r\n",
       "<!-- I1&#45;&gt;X -->\r\n",
       "<g id=\"edge8\" class=\"edge\">\r\n",
       "<title>I1&#45;&gt;X</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M104.33,-288.05C106.84,-280.06 109.9,-270.33 112.7,-261.4\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"116.06,-262.38 115.72,-251.79 109.39,-260.28 116.06,-262.38\"/>\r\n",
       "</g>\r\n",
       "<!-- I2 -->\r\n",
       "<g id=\"node8\" class=\"node\">\r\n",
       "<title>I2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171\" cy=\"-306\" rx=\"27\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-302.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">I2</text>\r\n",
       "</g>\r\n",
       "<!-- I2&#45;&gt;X -->\r\n",
       "<g id=\"edge9\" class=\"edge\">\r\n",
       "<title>I2&#45;&gt;X</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159.9,-289.46C153.49,-280.49 145.3,-269.02 138.08,-258.92\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"140.73,-256.59 132.07,-250.49 135.03,-260.66 140.73,-256.59\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1facb108e20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_example_dag = graphviz.Digraph('DAG')\n",
    "\n",
    "identity_example_dag.node('Y')\n",
    "identity_example_dag.node('X')\n",
    "identity_example_dag.node('M1')\n",
    "identity_example_dag.node('M2')\n",
    "identity_example_dag.node('C1')\n",
    "identity_example_dag.node('C2')\n",
    "identity_example_dag.node('I1')\n",
    "identity_example_dag.node('I2')\n",
    "\n",
    "\n",
    "identity_example_dag.edges([('X', 'M1'),\n",
    "           ('M1', 'M2'), \n",
    "           ('M2', 'Y'), \n",
    "           ('C1', 'X'),\n",
    "           ('C2', 'X'),\n",
    "           ('C1', 'Y'), \n",
    "           ('C2', 'Y'),\n",
    "           ('I1', 'X'), \n",
    "           ('I2', 'X'),\n",
    "])\n",
    "identity_example_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cx5EYrHFPGBZ"
   },
   "outputs": [],
   "source": [
    "identity_example_dag_for_model = identity_example_dag.source.replace(' DAG', '').replace('\\n', ';').replace('\\t', '').replace('{;Y', '{Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJk1n-iePGJF",
    "outputId": "479478c4-0bbb-48de-b89d-f1264c3f0388"
   },
   "outputs": [],
   "source": [
    "model_example = CausalModel(\n",
    "    data=pd.DataFrame(\n",
    "        {\n",
    "            'C1': [],\n",
    "            'C2': [],\n",
    "            'X': [],\n",
    "            'Y': [],\n",
    "            'M1': [],\n",
    "            'M2': [],\n",
    "            'I1': [],\n",
    "            'I2': [],\n",
    "    }),\n",
    "    treatment='X',\n",
    "    outcome='Y',\n",
    "    graph=identity_example_dag_for_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRYJLjkuPwYb",
    "outputId": "3df1fab2-89d2-43cf-e23c-0c4a16784b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimand type: nonparametric-ate\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: backdoor\n",
      "Estimand expression:\n",
      " d              \n",
      "────(E[Y|C2,C1])\n",
      "d[X]            \n",
      "Estimand assumption 1, Unconfoundedness: If U→{X} and U→Y then P(Y|X,C2,C1,U) = P(Y|X,C2,C1)\n",
      "\n",
      "### Estimand : 2\n",
      "Estimand name: iv\n",
      "Estimand expression:\n",
      " ⎡                             -1⎤\n",
      " ⎢    d        ⎛    d         ⎞  ⎥\n",
      "E⎢─────────(Y)⋅⎜─────────([X])⎟  ⎥\n",
      " ⎣d[I₂  I₁]    ⎝d[I₂  I₁]     ⎠  ⎦\n",
      "Estimand assumption 1, As-if-random: If U→→Y then ¬(U →→{I2,I1})\n",
      "Estimand assumption 2, Exclusion: If we remove {I2,I1}→{X}, then ¬({I2,I1}→Y)\n",
      "\n",
      "### Estimand : 3\n",
      "Estimand name: frontdoor\n",
      "Estimand expression:\n",
      " ⎡  d       d        ⎤\n",
      "E⎢─────(Y)⋅────([M₂])⎥\n",
      " ⎣d[M₂]    d[X]      ⎦\n",
      "Estimand assumption 1, Full-mediation: M2 intercepts (blocks) all directed paths from X to Y.\n",
      "Estimand assumption 2, First-stage-unconfoundedness: If U→{X} and U→{M2} then P(M2|X,U) = P(M2|X)\n",
      "Estimand assumption 3, Second-stage-unconfoundedness: If U→{M2} and U→Y then P(Y|M2, X, U) = P(Y|M2, X)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "identify_example = model_example.identify_effect(proceed_when_unidentifiable=True)\n",
    "print(identify_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SJ2FOEqOr4W"
   },
   "source": [
    "## **3. Estimate**\n",
    " \n",
    "There are a variety of ways to estimate the causal effect in `DoWhy`. We will explain the broad categories.\n",
    " \n",
    " \n",
    "### Linear model\n",
    " \n",
    "The linear model fits on the common causes and the treatment predicting on Y. It then extracts the causal effect from this model. This approach assume linear relationships between causes and Y.\n",
    " \n",
    "### Propensity scores\n",
    " \n",
    "A propensity score seeks to be a single value replacement that explains the effect of all the common causes. This single entity acts as a proxy for the common causes and makes calculating the backdoor more feasible. If there were many common causes (or high dimensional common causes), then the straightforward method of calculating the backdoor may not be practical. There are a number of individual strategies that fall under this broad category.\n",
    " \n",
    " \n",
    "### IV based strategies\n",
    " \n",
    "One is called regression discontinuity, where the model searches for some partition that separates the data between the two treatment values. It then uses values close to the threshold to estimate because values close to the partition  should have similar confounder values.\n",
    " \n",
    "### Double ML\n",
    " \n",
    "This method uses ML to predict the outcome from the common causes and predict the treatment from the common causes. It then combines these models to estimate the effect of the treatment on the outcome. Its strength is that these models can work well with many common causes that may be a challenge for backdoor estimation, but easy to make a model from. For example, if there are 50 common causes, then the integration over all of them for the backdoor strategy is just not feasible, but making an ML model with 50 features is not challenging. The flexibility in model choice is another benefit.\n",
    " \n",
    " \n",
    "## **4. Refute**\n",
    " \n",
    "Assumptions are made in modeling and estimating the causal effect. We would like some way to challenge our model's view of the causal effect.\n",
    " \n",
    "* **random common cause** This strategy generates a random common cause, for example one that may be unobserved or unmodeled. As a random common cause should have no effect on our estimate, adding it in should not change the estimated effect. [Docs](https://py-why.github.io/dowhy/v0.8/dowhy.causal_refuters.html?highlight=random_common_cause#module-dowhy.causal_refuters.random_common_cause)\n",
    " \n",
    "* **Placebo** This strategy replaces the treatment with a randomly generated variable. If the treatment variable is random, we should expect no detected causality on the outcome. Here, the model is good when the refutation effect is 0. [Docs](https://py-why.github.io/dowhy/v0.8/dowhy.causal_refuters.html?highlight=placebo#module-dowhy.causal_refuters.placebo_treatment_refuter)\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CausalML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
